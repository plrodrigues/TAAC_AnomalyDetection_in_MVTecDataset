{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom packages from src dir\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# python packages\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom packages\n",
    "import src.commons.dataset as ds\n",
    "import src.commons.constants as cons\n",
    "\n",
    "# Define the logging level\n",
    "logging.getLogger().setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer approach\n",
    "\n",
    "The ViT is an architecture that leverages transformers to image-based tasks. \n",
    "\n",
    "The main idea is to convert input image into a sequence of patches. We embed this sequence with a linear layer, plus positional encoding. Then, we apply a multi-head transformer layer. The output features are then used to solve the given task, e.g. classification.\n",
    "\n",
    "The idea is to frame our anomaly detection as a weakly supervised problem, where weak binary labels $y$ are given. $y = 0$ means the image is normal, $y=1$ means the image is anomalous. This is called *weak* supervision because no information about *where* the anomaly is (i.e. ground truth mask) is supplied.\n",
    "\n",
    "Then, we train a ViT in the task of predicting $\\hat{y}$. By extracting information from the attention layers inside the transformer, we should be able to obtain anomaly masks. This should be the case because the attention should learn to prioritize anomalous regions in order to perform the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader and data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cat = \"capsule\"\n",
    "data = ds.MVTECTestDataset(os.path.join(ds.current_dir(),'../', cons.DATA_PATH), cat)\n",
    "\n",
    "# Split data into train/test/val\n",
    "lengths = torch.floor(torch.tensor([0.8, 0.1, 0.1])*len(data))\n",
    "diff = torch.abs(lengths.sum() - len(data))\n",
    "if diff > 0:\n",
    "    lengths[0] += diff\n",
    "\n",
    "train_data, test_data, val_data = torch.utils.data.random_split(data, lengths.to(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 50\n",
    "\n",
    "def collate_fn(x):\n",
    "    ''' \n",
    "    x: list - batch_size\n",
    "    \n",
    "    Converts list of input tensors (C, W, H) to batch.\n",
    "    Normalizes each image from 0-255 to 0-1.\n",
    "    For each image, convert it into sequence of W/P vectors of dimension C*P**2,\n",
    "    where P is a fixed patch size.\n",
    "\n",
    "    output: Tensor - (batch_size, W/P, C*P**2)\n",
    "    '''   \n",
    "    # Extract input images and labels\n",
    "    imgs = [data[\"test\"] for data in x]\n",
    "\n",
    "    targets = [1 if torch.any(data[\"ground_truth\"] > 0) else 0\n",
    "               for data in x]\n",
    "\n",
    "    C, W, H = imgs[0].shape\n",
    "\n",
    "    # Convert images to 0-1\n",
    "    imgs = [img / 255.0 for img in imgs]\n",
    "\n",
    "    # Extract patches \n",
    "    patch_size = PATCH_SIZE\n",
    "    temp = []\n",
    "    for id, img in enumerate(imgs):\n",
    "        # Cut up image into flatenned sequence of patches\n",
    "        patches = [img[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size].flatten()\n",
    "                   for i in range(W//patch_size) for j in range(H//patch_size)]\n",
    "        patches = torch.stack(patches)\n",
    "        temp.append(patches)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    data, targets = torch.stack(temp), torch.tensor(targets)\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:07<00:00,  3.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders for batching input\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Check if loaders are working properly\n",
    "def test_dataloader(data_loader):\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for i, batch in pbar:   \n",
    "        data, targets = batch\n",
    "    return \n",
    "\n",
    "test_dataloader(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "# Define model\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, seq_len, input_features, num_encoder_layers, d_model, nhead):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(in_features=input_features, out_features=d_model)\n",
    "        self.pe_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model, nhead),\n",
    "                                                 num_layers=num_encoder_layers)\n",
    "        self.output = nn.Linear(in_features=d_model*seq_len, out_features=1)\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        logging.debug(f\"DTypes: raw input {type(x)}\")\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        logging.debug(f\"DTypes: linear output {type(x)}\")\n",
    "        logging.debug(f\"PE Encoder: input (after Linear) {x.shape}, permuted {torch.swapdims(x, 0, 1).shape}\")\n",
    "\n",
    "        u = self.pe_encoder(torch.swapdims(x, 0, 1))\n",
    "        logging.debug(f\"PE Encoder: output {u.shape}, permuted: {torch.swapdims(u, 0, 1).shape}\")\n",
    "        x += torch.swapdims(u, 0, 1)\n",
    "        x = self.encoder(x)\n",
    "        logging.debug(f\"Pre logits: {x.shape}\")\n",
    "        logits = self.output(torch.flatten(x, start_dim=1, end_dim=2))\n",
    "        logging.debug(f\"Logits: {logits.shape}\")\n",
    "        return logits.squeeze()\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        logging.debug(f\"DTypes: PE before droput output {type(x)}\")\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, **kwargs):\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.binary_cross_entropy_with_logits)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    model.eval() # set model to evaluation mode\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    avg_loss = 0.\n",
    "    for i, batch in pbar:\n",
    "        data, targets = batch\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        with torch.no_grad(): # no need to compute gradients\n",
    "            logits = model(data)\n",
    "        loss = loss_fn(logits, targets)\n",
    "        avg_loss += loss.item()\n",
    "        pbar.set_description(f'loss = {loss:.3f}')\n",
    "    avg_loss /= len(data_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, **kwargs):\n",
    "    num_epochs = kwargs.get('num_epochs', 100)\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.mse_loss)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        print('Training phase...')\n",
    "        model.train() # set model to training mode\n",
    "        train_loss = 0.\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for i, batch in pbar:\n",
    "            data, targets = batch\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            model.zero_grad() # initialize gradients to zero\n",
    "            logits = model(data) # forward pass\n",
    "            logging.debug(f\"Before loss computation (training), logits: {logits.shape} targets: {targets.shape}\")\n",
    "            logging.debug(f\"DTypes: logits {type(logits)}, targets {type(targets)}\")\n",
    "            loss = loss_fn(logits, targets) # loss computation\n",
    "            loss.backward() # computing gradients (backward pass)\n",
    "            optimizer.step() # updating the parameters of the model\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            pbar.set_description(f'loss = {loss:.3f}')\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'train loss = {train_loss:.3f}')\n",
    "        train_loss_hist.append(train_loss)\n",
    "        \n",
    "        print('Validation phase...')\n",
    "        val_loss = evaluate(model, val_loader, loss_fn=loss_fn, device=device)\n",
    "        print(f'validation loss = {val_loss:.3f}')\n",
    "        val_loss_hist.append(val_loss)\n",
    "        \n",
    "    return train_loss_hist, val_loss_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "SEQUENCE_LENGTH = 400\n",
    "INPUT_FEATURES = 7500\n",
    "ENCODER_LAYERS = 3\n",
    "ENCODER_DIM = 128\n",
    "ENCODER_HEADS = 8\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('DEVICE:', DEVICE)\n",
    "\n",
    "model = VisionTransformer(SEQUENCE_LENGTH, INPUT_FEATURES, ENCODER_LAYERS, ENCODER_DIM, ENCODER_HEADS).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Training phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/27 [00:00<?, ?it/s]DEBUG:root:DTypes: raw input <class 'torch.Tensor'>\n",
      "DEBUG:root:DTypes: linear output <class 'torch.Tensor'>\n",
      "DEBUG:root:PE Encoder: input (after Linear) torch.Size([4, 400, 128]), permuted torch.Size([400, 4, 128])\n",
      "DEBUG:root:DTypes: PE before droput output <class 'torch.Tensor'>\n",
      "DEBUG:root:PE Encoder: output torch.Size([400, 4, 128]), permuted: torch.Size([4, 400, 128])\n",
      "DEBUG:root:Pre logits: torch.Size([4, 400, 128])\n",
      "DEBUG:root:Logits: torch.Size([4, 1])\n",
      "DEBUG:root:Before loss computation (training), logits: torch.Size([4]) targets: torch.Size([4])\n",
      "DEBUG:root:DTypes: logits <class 'torch.Tensor'>, targets <class 'torch.Tensor'>\n",
      "  0%|          | 0/27 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Long but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_loss, val_loss \u001b[39m=\u001b[39m fit(model, train_dataloader, val_dataloader, optimizer, num_epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, device\u001b[39m=\u001b[39;49mDEVICE)\n",
      "Cell \u001b[1;32mIn [58], line 40\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, optimizer, **kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDTypes: logits \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(logits)\u001b[39m}\u001b[39;00m\u001b[39m, targets \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(targets)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, targets) \u001b[39m# loss computation\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# computing gradients (backward pass)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# updating the parameters of the model\u001b[39;00m\n\u001b[0;32m     43\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Felipe\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Felipe\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"
     ]
    }
   ],
   "source": [
    "train_loss, val_loss = fit(model, train_dataloader, val_dataloader, optimizer, num_epochs=NUM_EPOCHS, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea628a803fd91925100a64391271027fefee9b19b057af296aad7ee8973386a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
