{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom packages from src dir\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "# python packages\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "# custom packages\n",
    "import src.commons.dataset as ds\n",
    "import src.commons.constants as cons\n",
    "\n",
    "# Define the logging level\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer approach\n",
    "\n",
    "The ViT is an architecture that leverages transformers to image-based tasks. \n",
    "\n",
    "The main idea is to convert input image into a sequence of patches. We embed this sequence with a linear layer, plus positional encoding. Then, we apply a multi-head transformer layer. The output features are then used to solve the given task, e.g. classification.\n",
    "\n",
    "The idea is to frame our anomaly detection as a weakly supervised problem, where weak binary labels $y$ are given. $y = 0$ means the image is normal, $y=1$ means the image is anomalous. This is called *weak* supervision because no information about *where* the anomaly is (i.e. ground truth mask) is supplied.\n",
    "\n",
    "Then, we train a ViT in the task of predicting $\\hat{y}$. By extracting information from the attention layers inside the transformer, we should be able to obtain anomaly masks. This should be the case because the attention should learn to prioritize anomalous regions in order to perform the classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "cat = \"capsule\"\n",
    "data = ds.MVTECTestDataset(os.path.join(ds.current_dir(),'../', cons.DATA_PATH), cat)\n",
    "\n",
    "# Split data into train/test/val\n",
    "lengths = torch.floor(torch.tensor([0.8, 0.1, 0.1])*len(data))\n",
    "diff = torch.abs(lengths.sum() - len(data))\n",
    "if diff > 0:\n",
    "    lengths[0] += diff\n",
    "\n",
    "train_data, test_data, val_data = torch.utils.data.random_split(data, lengths.to(int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 50\n",
    "\n",
    "def collate_fn(x):\n",
    "    ''' \n",
    "    x: list - batch_size\n",
    "    \n",
    "    Converts list of input tensors (C, W, H) to batch.\n",
    "    Normalizes each image from 0-255 to 0-1.\n",
    "    For each image, convert it into sequence of W/P vectors of dimension C*P**2,\n",
    "    where P is a fixed patch size.\n",
    "\n",
    "    output: Tensor - (batch_size, W/P, C*P**2)\n",
    "    '''   \n",
    "    # Extract input images and labels\n",
    "    imgs = [data[\"test\"] for data in x]\n",
    "\n",
    "    targets = [1 if torch.any(data[\"ground_truth\"] > 0) else 0\n",
    "               for data in x]\n",
    "\n",
    "    C, W, H = imgs[0].shape\n",
    "\n",
    "    # Convert images to 0-1\n",
    "    imgs = [img / 255.0 for img in imgs]\n",
    "\n",
    "    # Extract patches \n",
    "    patch_size = PATCH_SIZE\n",
    "    temp = []\n",
    "    for id, img in enumerate(imgs):\n",
    "        # Cut up image into flatenned sequence of patches\n",
    "        patches = [img[:, i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size].flatten()\n",
    "                   for i in range(W//patch_size) for j in range(H//patch_size)]\n",
    "        patches = torch.stack(patches)\n",
    "        temp.append(patches)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    data, targets = torch.stack(temp), torch.tensor(targets)\n",
    "    return data, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:04<00:00,  5.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders for batching input\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "# Check if loaders are working properly\n",
    "def test_dataloader(data_loader):\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    for i, batch in pbar:   \n",
    "        data, targets = batch\n",
    "    return \n",
    "\n",
    "test_dataloader(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, **kwargs):\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.mse_loss)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    model.eval() # set model to evaluation mode\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n",
    "    avg_loss = 0.\n",
    "    for i, batch in pbar:\n",
    "        ## YOUR CODE HERE ##\n",
    "        batch[\"inputs\"] = batch[\"inputs\"].to(device)\n",
    "        batch[\"targets\"] = batch[\"targets\"].to(device)\n",
    "        num_steps = batch[\"targets\"].shape[1] # batch_size, time, dim\n",
    "        with torch.no_grad(): # no need to compute gradients\n",
    "            preds = model(batch[\"inputs\"], num_steps)\n",
    "        loss = loss_fn(preds, batch[\"targets\"])\n",
    "        avg_loss += loss.item()\n",
    "        ## *** ##\n",
    "        pbar.set_description(f'loss = {loss:.3f}')\n",
    "    avg_loss /= len(val_loader)\n",
    "    return avg_loss\n",
    "\n",
    "def fit(model, train_loader, val_loader, optimizer, **kwargs):\n",
    "    num_epochs = kwargs.get('num_epochs', 100)\n",
    "    loss_fn = kwargs.get('loss_fn', torch.nn.functional.mse_loss)\n",
    "    device = kwargs.get('device', torch.device('cpu'))\n",
    "    \n",
    "    train_loss_hist, val_loss_hist = [], []\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        \n",
    "        print('Training phase...')\n",
    "        model.train() # set model to training mode\n",
    "        train_loss = 0.\n",
    "        pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "        for i, batch in pbar:\n",
    "            ## YOUR CODE HERE ##\n",
    "            batch[\"inputs\"] = batch[\"inputs\"].to(device)\n",
    "            batch[\"targets\"] = batch[\"targets\"].to(device)\n",
    "            model.zero_grad() # initialize gradients to zero\n",
    "            num_steps = batch[\"targets\"].shape[1] \n",
    "            preds = model(batch[\"inputs\"], num_steps) # forward pass\n",
    "            loss = loss_fn(preds, batch[\"targets\"]) # loss computation\n",
    "            loss.backward() # computing gradients (backward pass)\n",
    "            optimizer.step() # updating the parameters of the model\n",
    "            ## *** ##\n",
    "            train_loss += loss.item()\n",
    "            pbar.set_description(f'loss = {loss:.3f}')\n",
    "        train_loss /= len(train_loader)\n",
    "        print(f'train loss = {train_loss:.3f}')\n",
    "        train_loss_hist.append(train_loss)\n",
    "        \n",
    "        print('Validation phase...')\n",
    "        val_loss = evaluate(model, val_loader, loss_fn=loss_fn, device=device)\n",
    "        print(f'validation loss = {val_loss:.3f}')\n",
    "        val_loss_hist.append(val_loss)\n",
    "        \n",
    "    return train_loss_hist, val_loss_hist\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea628a803fd91925100a64391271027fefee9b19b057af296aad7ee8973386a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
